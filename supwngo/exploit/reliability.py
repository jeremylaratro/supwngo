"""
Exploit reliability and stability enhancements.

Provides techniques to make exploits more reliable in real-world
conditions, handling ASLR, heap randomization, race conditions,
and other sources of non-determinism.

Features:
- Heap feng shui optimization
- ASLR entropy reduction
- Race condition handling
- Memory spray techniques
- Retry and fallback strategies
- Exploit chain validation
"""

import random
import struct
import time
from dataclasses import dataclass, field
from enum import Enum, auto
from typing import Any, Callable, Dict, List, Optional, Tuple, Union

from supwngo.utils.logging import get_logger

logger = get_logger(__name__)


class ReliabilityTechnique(Enum):
    """Reliability enhancement techniques."""
    HEAP_SPRAY = auto()          # Flood heap with controlled data
    HEAP_FENG_SHUI = auto()      # Precise heap layout control
    INFO_LEAK = auto()           # Reduce entropy via leaks
    RACE_RETRY = auto()          # Retry-based race exploitation
    MEMORY_MASSAGE = auto()      # Prepare memory state
    ENTROPY_REDUCTION = auto()   # Reduce ASLR entropy
    OFFSET_BRUTEFORCE = auto()   # Brute force unknown offsets
    TIMING_BASED = auto()        # Timing-dependent exploitation


@dataclass
class ReliabilityConfig:
    """Configuration for reliability enhancements."""
    max_retries: int = 100
    retry_delay: float = 0.1
    heap_spray_count: int = 1000
    race_window_us: int = 100
    bruteforce_limit: int = 4096
    timeout_seconds: int = 30
    enable_logging: bool = True


@dataclass
class ExploitAttempt:
    """Record of a single exploit attempt."""
    attempt_number: int
    success: bool
    duration: float
    technique_used: ReliabilityTechnique
    error: Optional[str] = None
    leak_value: int = 0


@dataclass
class ReliabilityReport:
    """Complete reliability analysis report."""
    total_attempts: int
    successful_attempts: int
    success_rate: float
    average_time: float
    techniques_used: List[ReliabilityTechnique]
    attempts: List[ExploitAttempt] = field(default_factory=list)


class HeapSpray:
    """
    Heap spraying for exploit reliability.

    Sprays controlled data across the heap to increase
    the probability of hitting valid memory.
    """

    def __init__(
        self,
        spray_size: int = 0x1000,
        nop_sled: bytes = b"\x90",
        alignment: int = 16,
    ):
        self.spray_size = spray_size
        self.nop_sled = nop_sled
        self.alignment = alignment

    def generate_spray_block(
        self,
        target_addr: int,
        payload: bytes,
        arch: str = "amd64",
    ) -> bytes:
        """
        Generate a single spray block.

        Args:
            target_addr: Expected landing address
            payload: Exploit payload
            arch: Target architecture

        Returns:
            Spray block bytes
        """
        # Calculate NOP sled size
        nop_size = self.spray_size - len(payload) - self.alignment

        if nop_size <= 0:
            return payload

        # Build spray block
        block = self.nop_sled * nop_size + payload

        # Align to boundary
        padding = self.alignment - (len(block) % self.alignment)
        block += b"\x00" * padding

        return block

    def generate_spray(
        self,
        count: int,
        target_addr: int,
        payload: bytes,
    ) -> List[bytes]:
        """
        Generate multiple spray blocks.

        Args:
            count: Number of blocks
            target_addr: Expected landing address
            payload: Exploit payload

        Returns:
            List of spray blocks
        """
        blocks = []
        for i in range(count):
            block = self.generate_spray_block(
                target_addr + (i * self.spray_size),
                payload,
            )
            blocks.append(block)
        return blocks

    def calculate_coverage(
        self,
        start_addr: int,
        end_addr: int,
        block_count: int,
    ) -> float:
        """Calculate memory coverage percentage."""
        total_range = end_addr - start_addr
        covered = block_count * self.spray_size
        return min(1.0, covered / total_range)


class HeapFengShui:
    """
    Precise heap layout manipulation.

    Arranges heap allocations to create predictable layouts
    for exploitation, especially for use-after-free.
    """

    @dataclass
    class AllocationSlot:
        """A slot in the heap layout."""
        index: int
        size: int
        data: Optional[bytes] = None
        freed: bool = False
        purpose: str = ""

    def __init__(self):
        self.slots: List[HeapFengShui.AllocationSlot] = []
        self.target_size: int = 0
        self.allocator: str = "glibc"

    def plan_layout(
        self,
        target_size: int,
        victim_before: int = 5,
        victim_after: int = 5,
        padding_size: int = 0x100,
    ) -> List['HeapFengShui.AllocationSlot']:
        """
        Plan heap layout for exploitation.

        Args:
            target_size: Size of vulnerable allocation
            victim_before: Number of allocations before target
            victim_after: Number of allocations after target
            padding_size: Size of padding allocations

        Returns:
            List of planned allocation slots
        """
        self.target_size = target_size
        self.slots = []

        # Create padding allocations before target
        for i in range(victim_before):
            self.slots.append(self.AllocationSlot(
                index=i,
                size=padding_size,
                purpose="padding_before",
            ))

        # Target allocation slot
        target_idx = victim_before
        self.slots.append(self.AllocationSlot(
            index=target_idx,
            size=target_size,
            purpose="target",
        ))

        # Create padding allocations after target
        for i in range(victim_after):
            self.slots.append(self.AllocationSlot(
                index=target_idx + 1 + i,
                size=padding_size,
                purpose="padding_after",
            ))

        return self.slots

    def plan_tcache_poisoning(
        self,
        target_size: int,
        fake_chunk_addr: int,
    ) -> Dict[str, Any]:
        """
        Plan heap layout for tcache poisoning.

        Args:
            target_size: Chunk size to poison
            fake_chunk_addr: Address of fake chunk

        Returns:
            Layout plan dictionary
        """
        # Round to tcache bin size
        if target_size < 0x20:
            target_size = 0x20
        bin_size = ((target_size + 0x17) // 0x10) * 0x10

        return {
            "bin_size": bin_size,
            "steps": [
                {"action": "alloc", "size": bin_size, "id": "A"},
                {"action": "alloc", "size": bin_size, "id": "B"},
                {"action": "free", "id": "B"},
                {"action": "free", "id": "A"},  # tcache: A -> B
                {"action": "overwrite", "id": "A", "target": fake_chunk_addr},
                {"action": "alloc", "size": bin_size, "id": "C"},  # Gets A
                {"action": "alloc", "size": bin_size, "id": "D"},  # Gets fake
            ],
            "fake_chunk_addr": fake_chunk_addr,
        }

    def plan_fastbin_dup(
        self,
        target_size: int,
        fake_chunk_addr: int,
    ) -> Dict[str, Any]:
        """
        Plan heap layout for fastbin dup attack.

        Args:
            target_size: Chunk size to poison
            fake_chunk_addr: Address of fake chunk

        Returns:
            Layout plan dictionary
        """
        if target_size < 0x20:
            target_size = 0x20
        if target_size >= 0x80:
            target_size = 0x70

        return {
            "bin_size": target_size,
            "steps": [
                {"action": "alloc", "size": target_size, "id": "A"},
                {"action": "alloc", "size": target_size, "id": "B"},
                {"action": "alloc", "size": target_size, "id": "C"},
                {"action": "free", "id": "A"},
                {"action": "free", "id": "B"},
                {"action": "free", "id": "A"},  # Double free: A -> B -> A
                {"action": "alloc", "size": target_size, "id": "D", "data": fake_chunk_addr},
                {"action": "alloc", "size": target_size, "id": "E"},
                {"action": "alloc", "size": target_size, "id": "F"},  # Gets A again
                {"action": "alloc", "size": target_size, "id": "G"},  # Gets fake
            ],
            "fake_chunk_addr": fake_chunk_addr,
        }

    def plan_uaf_overlap(
        self,
        target_size: int,
        overlap_size: int,
    ) -> Dict[str, Any]:
        """
        Plan heap layout for UAF with overlapping allocation.

        Args:
            target_size: Size of freed chunk
            overlap_size: Size of overlapping object

        Returns:
            Layout plan
        """
        return {
            "target_size": target_size,
            "overlap_size": overlap_size,
            "steps": [
                {"action": "alloc", "size": target_size, "id": "victim"},
                {"action": "free", "id": "victim"},
                {"action": "alloc", "size": overlap_size, "id": "overlay"},
                # Now victim and overlay point to same memory
                {"action": "use_dangling", "id": "victim"},
            ],
        }


class RaceConditionHandler:
    """
    Handle race condition exploitation.

    Provides techniques for reliable race condition exploitation
    including retry logic and timing optimization.
    """

    def __init__(self, config: Optional[ReliabilityConfig] = None):
        self.config = config or ReliabilityConfig()
        self.attempts: List[ExploitAttempt] = []
        self.timing_samples: List[float] = []

    def race_retry(
        self,
        exploit_func: Callable[[], bool],
        max_attempts: int = 0,
    ) -> Tuple[bool, int]:
        """
        Retry race condition exploit until success.

        Args:
            exploit_func: Function that returns True on success
            max_attempts: Maximum attempts (0 = use config)

        Returns:
            (success, attempts_needed)
        """
        if max_attempts == 0:
            max_attempts = self.config.max_retries

        for i in range(max_attempts):
            start_time = time.time()
            try:
                success = exploit_func()
                duration = time.time() - start_time

                attempt = ExploitAttempt(
                    attempt_number=i,
                    success=success,
                    duration=duration,
                    technique_used=ReliabilityTechnique.RACE_RETRY,
                )
                self.attempts.append(attempt)
                self.timing_samples.append(duration)

                if success:
                    logger.info(f"Race won after {i+1} attempts")
                    return True, i + 1

            except Exception as e:
                attempt = ExploitAttempt(
                    attempt_number=i,
                    success=False,
                    duration=time.time() - start_time,
                    technique_used=ReliabilityTechnique.RACE_RETRY,
                    error=str(e),
                )
                self.attempts.append(attempt)

            time.sleep(self.config.retry_delay)

        return False, max_attempts

    def calibrate_timing(
        self,
        operation: Callable[[], None],
        samples: int = 100,
    ) -> Dict[str, float]:
        """
        Calibrate timing for race conditions.

        Args:
            operation: Operation to time
            samples: Number of samples

        Returns:
            Timing statistics
        """
        times = []

        for _ in range(samples):
            start = time.perf_counter_ns()
            operation()
            end = time.perf_counter_ns()
            times.append(end - start)

        return {
            "min_ns": min(times),
            "max_ns": max(times),
            "avg_ns": sum(times) / len(times),
            "median_ns": sorted(times)[len(times) // 2],
            "samples": samples,
        }

    def optimize_race_window(
        self,
        measure_func: Callable[[], float],
        adjust_func: Callable[[float], None],
        target_window_ns: int = 1000,
    ) -> float:
        """
        Optimize timing for race window.

        Args:
            measure_func: Returns current timing
            adjust_func: Adjusts timing parameter
            target_window_ns: Target race window in nanoseconds

        Returns:
            Optimized delay value
        """
        delay = 0.0
        step = 0.0001  # 100us

        for _ in range(100):
            measured = measure_func()
            if abs(measured - target_window_ns) < 100:
                break

            if measured > target_window_ns:
                delay -= step
            else:
                delay += step

            adjust_func(delay)

        return delay


class EntropyReducer:
    """
    Reduce ASLR and other entropy.

    Techniques for reducing randomization entropy to
    make exploits more reliable.
    """

    # Common ASLR entropy bits by platform
    ENTROPY_BITS = {
        "linux_x64_stack": 30,
        "linux_x64_heap": 28,
        "linux_x64_mmap": 28,
        "linux_x64_pie": 28,
        "linux_x86_stack": 24,
        "linux_x86_heap": 13,
        "linux_x86_mmap": 8,
        "windows_x64_stack": 17,
        "windows_x64_heap": 17,
        "windows_x86_stack": 14,
    }

    def __init__(self, arch: str = "amd64"):
        self.arch = arch
        self.known_bits: Dict[str, int] = {}

    def leak_to_entropy_reduction(
        self,
        leak_addr: int,
        leak_type: str = "heap",
    ) -> int:
        """
        Calculate entropy reduction from a leak.

        Args:
            leak_addr: Leaked address
            leak_type: Type of address (heap, stack, libc, etc.)

        Returns:
            Bits of entropy eliminated
        """
        if self.arch == "amd64":
            # Canonical address gives us top 16 bits
            canonical_bits = 16

            # Page alignment gives us 12 bits
            page_bits = 12

            # Total known bits
            total = canonical_bits + page_bits

            return total
        elif self.arch == "i386":
            # 32-bit is much easier
            return 24  # Only 8 bits of true entropy typically

        return 0

    def calculate_bruteforce_attempts(
        self,
        unknown_bits: int,
        success_probability: float = 1.0,
    ) -> int:
        """
        Calculate expected brute force attempts needed.

        Args:
            unknown_bits: Bits of unknown entropy
            success_probability: Expected success rate per attempt

        Returns:
            Expected number of attempts
        """
        total_space = 2 ** unknown_bits
        return int(total_space / success_probability / 2)

    def partial_overwrite_entropy(
        self,
        overwrite_bytes: int = 2,
    ) -> int:
        """
        Calculate entropy when doing partial pointer overwrite.

        Args:
            overwrite_bytes: Number of bytes being overwritten

        Returns:
            Bits of entropy in overwritten portion
        """
        # Only the overwritten portion is random
        # But page alignment helps
        raw_bits = overwrite_bytes * 8
        aligned_bits = raw_bits - 12  # Subtract page offset bits

        return max(0, aligned_bits)


class ExploitValidator:
    """
    Validate exploit chains before execution.

    Checks that all primitives are available and
    constraints are satisfied.
    """

    @dataclass
    class ValidationResult:
        """Result of exploit validation."""
        valid: bool
        errors: List[str] = field(default_factory=list)
        warnings: List[str] = field(default_factory=list)
        requirements_met: Dict[str, bool] = field(default_factory=dict)

    def __init__(self):
        self.requirements: List[str] = []
        self.checks: Dict[str, Callable[[], bool]] = {}

    def add_requirement(
        self,
        name: str,
        check_func: Callable[[], bool],
    ):
        """Add a requirement check."""
        self.requirements.append(name)
        self.checks[name] = check_func

    def validate(self) -> 'ExploitValidator.ValidationResult':
        """
        Run all validation checks.

        Returns:
            ValidationResult
        """
        result = self.ValidationResult(valid=True)

        for name in self.requirements:
            try:
                passed = self.checks[name]()
                result.requirements_met[name] = passed
                if not passed:
                    result.errors.append(f"Requirement failed: {name}")
                    result.valid = False
            except Exception as e:
                result.requirements_met[name] = False
                result.errors.append(f"Check raised exception: {name}: {e}")
                result.valid = False

        return result

    @staticmethod
    def check_leak_quality(
        leak: int,
        expected_prefix: int = 0x7f,
        arch: str = "amd64",
    ) -> bool:
        """Check if a leak looks valid."""
        if arch == "amd64":
            # Check canonical address
            top_byte = (leak >> 56) & 0xFF
            if top_byte not in (0x00, 0x7f, 0xff):
                return False
            # Check alignment
            if leak & 0xF != 0:
                return False
            return True
        elif arch == "i386":
            # 32-bit addresses
            if leak > 0xFFFFFFFF:
                return False
            return True
        return False


class RetryStrategy:
    """
    Retry strategies for unreliable exploits.

    Implements various retry patterns for different
    failure modes.
    """

    def __init__(self, config: Optional[ReliabilityConfig] = None):
        self.config = config or ReliabilityConfig()

    def exponential_backoff(
        self,
        exploit_func: Callable[[], bool],
        base_delay: float = 0.1,
        max_delay: float = 10.0,
        max_attempts: int = 0,
    ) -> Tuple[bool, int]:
        """
        Retry with exponential backoff.

        Args:
            exploit_func: Function to retry
            base_delay: Initial delay between attempts
            max_delay: Maximum delay
            max_attempts: Maximum attempts (0 = use config)

        Returns:
            (success, attempts)
        """
        if max_attempts == 0:
            max_attempts = self.config.max_retries

        delay = base_delay

        for i in range(max_attempts):
            try:
                if exploit_func():
                    return True, i + 1
            except Exception:
                pass

            time.sleep(min(delay, max_delay))
            delay *= 2

        return False, max_attempts

    def jittered_retry(
        self,
        exploit_func: Callable[[], bool],
        base_delay: float = 0.1,
        jitter: float = 0.5,
        max_attempts: int = 0,
    ) -> Tuple[bool, int]:
        """
        Retry with randomized timing (helps with races).

        Args:
            exploit_func: Function to retry
            base_delay: Base delay between attempts
            jitter: Random factor (0.5 = Â±50%)
            max_attempts: Maximum attempts

        Returns:
            (success, attempts)
        """
        if max_attempts == 0:
            max_attempts = self.config.max_retries

        for i in range(max_attempts):
            try:
                if exploit_func():
                    return True, i + 1
            except Exception:
                pass

            actual_delay = base_delay * (1 + random.uniform(-jitter, jitter))
            time.sleep(max(0, actual_delay))

        return False, max_attempts

    def parallel_attempt(
        self,
        exploit_func: Callable[[], bool],
        workers: int = 4,
        timeout: float = 0,
    ) -> Tuple[bool, int]:
        """
        Run parallel exploit attempts.

        Args:
            exploit_func: Function to run
            workers: Number of parallel workers
            timeout: Timeout for each worker

        Returns:
            (success, total_attempts)
        """
        import concurrent.futures

        if timeout == 0:
            timeout = self.config.timeout_seconds

        attempts = 0
        with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
            futures = [
                executor.submit(exploit_func)
                for _ in range(workers)
            ]

            for future in concurrent.futures.as_completed(futures, timeout=timeout):
                attempts += 1
                try:
                    if future.result():
                        # Cancel remaining
                        for f in futures:
                            f.cancel()
                        return True, attempts
                except Exception:
                    pass

        return False, attempts


def analyze_reliability(
    exploit_func: Callable[[], bool],
    iterations: int = 100,
    config: Optional[ReliabilityConfig] = None,
) -> ReliabilityReport:
    """
    Analyze exploit reliability through repeated execution.

    Args:
        exploit_func: Exploit function
        iterations: Number of test iterations
        config: Reliability configuration

    Returns:
        ReliabilityReport with statistics
    """
    if config is None:
        config = ReliabilityConfig()

    report = ReliabilityReport(
        total_attempts=iterations,
        successful_attempts=0,
        success_rate=0.0,
        average_time=0.0,
        techniques_used=[],
    )

    total_time = 0.0

    for i in range(iterations):
        start = time.time()
        try:
            success = exploit_func()
            duration = time.time() - start
            total_time += duration

            attempt = ExploitAttempt(
                attempt_number=i,
                success=success,
                duration=duration,
                technique_used=ReliabilityTechnique.HEAP_FENG_SHUI,
            )
            report.attempts.append(attempt)

            if success:
                report.successful_attempts += 1

        except Exception as e:
            duration = time.time() - start
            total_time += duration

            attempt = ExploitAttempt(
                attempt_number=i,
                success=False,
                duration=duration,
                technique_used=ReliabilityTechnique.HEAP_FENG_SHUI,
                error=str(e),
            )
            report.attempts.append(attempt)

    report.success_rate = report.successful_attempts / report.total_attempts
    report.average_time = total_time / report.total_attempts

    logger.info(
        f"Reliability: {report.success_rate:.1%} "
        f"({report.successful_attempts}/{report.total_attempts})"
    )

    return report
